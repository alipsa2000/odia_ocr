{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from tensorflow.python.keras.layers import Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable eager execution\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL VARIABLES\n",
    "path = \"data1/NITROHCS_V1.0/\"\n",
    "Test_Ratio = 0.2\n",
    "ValRatio = 0.2\n",
    "images = []\n",
    "classid = []\n",
    "inputShape = (32, 32, 3)\n",
    "batchsizeVal = 64\n",
    "epochsVal = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15040 files belonging to 47 classes.\n",
      "Using 12032 files for training.\n",
      "Found 15040 files belonging to 47 classes.\n",
      "Using 3008 files for validation.\n",
      "['NITROHCS-001', 'NITROHCS-002', 'NITROHCS-003', 'NITROHCS-004', 'NITROHCS-005', 'NITROHCS-006', 'NITROHCS-007', 'NITROHCS-008', 'NITROHCS-009', 'NITROHCS-010', 'NITROHCS-011', 'NITROHCS-012', 'NITROHCS-013', 'NITROHCS-014', 'NITROHCS-015', 'NITROHCS-016', 'NITROHCS-017', 'NITROHCS-018', 'NITROHCS-019', 'NITROHCS-020', 'NITROHCS-021', 'NITROHCS-022', 'NITROHCS-023', 'NITROHCS-024', 'NITROHCS-025', 'NITROHCS-026', 'NITROHCS-027', 'NITROHCS-028', 'NITROHCS-029', 'NITROHCS-030', 'NITROHCS-031', 'NITROHCS-032', 'NITROHCS-033', 'NITROHCS-034', 'NITROHCS-035', 'NITROHCS-036', 'NITROHCS-037', 'NITROHCS-038', 'NITROHCS-039', 'NITROHCS-040', 'NITROHCS-041', 'NITROHCS-042', 'NITROHCS-043', 'NITROHCS-044', 'NITROHCS-045', 'NITROHCS-046', 'NITROHCS-047']\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  path,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(inputShape[0],inputShape[1]),\n",
    "  batch_size=batchsizeVal)\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  path,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(inputShape[0],inputShape[1]),\n",
    "  batch_size=batchsizeVal)\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "# After eager execution is enabled, operations are executed as they are\n",
    "# defined and Tensor objects hold concrete values, which can be accessed as\n",
    "# numpy.ndarray`s through the numpy() method.\n",
    "# print(type(image))\n",
    "def preprocessing(image):\n",
    "    \n",
    "    image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "    image = cv2.equalizeHist(image)\n",
    "    image = image/255\n",
    "    return image\n",
    "\n",
    "# train_ds = train_ds.map(preprocessing)\n",
    "# val_ds = val_ds.map(preprocessing)\n",
    "for train_images in train_ds:\n",
    "    map(preprocessing,train_images)\n",
    "    \n",
    "   \n",
    "for val_images in val_ds:\n",
    "    map(preprocessing,val_images)\n",
    "#     print(val_images)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94668760/94668760 [==============================] - 26s 0us/step\n"
     ]
    }
   ],
   "source": [
    "resnet_model = Sequential()\n",
    "\n",
    "pretrained_model = tf.keras.applications.ResNet50V2(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=inputShape,\n",
    "    pooling='avg',\n",
    "    classes=len(class_names),\n",
    "    classifier_activation=\"softmax\"\n",
    ")\n",
    "\n",
    "for layer in pretrained_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "resnet_model.add(pretrained_model)\n",
    "resnet_model.add(Flatten())\n",
    "resnet_model.add(Dense(1024, activation='relu'))\n",
    "resnet_model.add(Dense(512, activation='relu'))\n",
    "resnet_model.add(Dense(512, activation='relu'))\n",
    "resnet_model.add(Dense(len(class_names), activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
      "                                                                 \n",
      " module_wrapper (ModuleWrapp  (None, 2048)             0         \n",
      " er)                                                             \n",
      "                                                                 \n",
      " module_wrapper_1 (ModuleWra  (None, 1024)             2098176   \n",
      " pper)                                                           \n",
      "                                                                 \n",
      " module_wrapper_2 (ModuleWra  (None, 512)              524800    \n",
      " pper)                                                           \n",
      "                                                                 \n",
      " module_wrapper_3 (ModuleWra  (None, 512)              262656    \n",
      " pper)                                                           \n",
      "                                                                 \n",
      " module_wrapper_4 (ModuleWra  (None, 47)               24111     \n",
      " pper)                                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26,474,543\n",
      "Trainable params: 2,909,743\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ALIPSA SAHOO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "resnet_model.compile(optimizer=Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "188/188 [==============================] - 57s 300ms/step - loss: 2.5970 - accuracy: 0.2745 - val_loss: 2.6557 - val_accuracy: 0.2523\n",
      "Epoch 2/50\n",
      "188/188 [==============================] - 58s 306ms/step - loss: 2.4741 - accuracy: 0.2881 - val_loss: 2.5031 - val_accuracy: 0.2959\n",
      "Epoch 3/50\n",
      "188/188 [==============================] - 58s 307ms/step - loss: 2.3233 - accuracy: 0.3265 - val_loss: 2.3552 - val_accuracy: 0.3132\n",
      "Epoch 4/50\n",
      "188/188 [==============================] - 59s 311ms/step - loss: 2.1593 - accuracy: 0.3691 - val_loss: 2.3134 - val_accuracy: 0.3348\n",
      "Epoch 5/50\n",
      "188/188 [==============================] - 58s 310ms/step - loss: 2.0871 - accuracy: 0.3851 - val_loss: 2.0948 - val_accuracy: 0.3886\n",
      "Epoch 6/50\n",
      "188/188 [==============================] - 73s 388ms/step - loss: 1.9592 - accuracy: 0.4197 - val_loss: 1.9708 - val_accuracy: 0.4189\n",
      "Epoch 7/50\n",
      "188/188 [==============================] - 63s 334ms/step - loss: 1.8540 - accuracy: 0.4461 - val_loss: 1.9903 - val_accuracy: 0.4039\n",
      "Epoch 8/50\n",
      "188/188 [==============================] - 57s 303ms/step - loss: 1.8359 - accuracy: 0.4480 - val_loss: 1.8758 - val_accuracy: 0.4292\n",
      "Epoch 9/50\n",
      "188/188 [==============================] - 58s 306ms/step - loss: 1.7114 - accuracy: 0.4868 - val_loss: 1.8485 - val_accuracy: 0.4461\n",
      "Epoch 10/50\n",
      "188/188 [==============================] - 58s 307ms/step - loss: 1.6782 - accuracy: 0.4899 - val_loss: 2.0728 - val_accuracy: 0.4053\n",
      "Epoch 11/50\n",
      "188/188 [==============================] - 58s 310ms/step - loss: 1.6728 - accuracy: 0.4910 - val_loss: 1.8091 - val_accuracy: 0.4634\n",
      "Epoch 12/50\n",
      "188/188 [==============================] - 60s 316ms/step - loss: 1.5690 - accuracy: 0.5250 - val_loss: 1.9477 - val_accuracy: 0.4368\n",
      "Epoch 13/50\n",
      "188/188 [==============================] - 55s 291ms/step - loss: 1.5661 - accuracy: 0.5199 - val_loss: 1.7203 - val_accuracy: 0.4870\n",
      "Epoch 14/50\n",
      "188/188 [==============================] - 52s 276ms/step - loss: 1.4550 - accuracy: 0.5489 - val_loss: 1.5824 - val_accuracy: 0.5170\n",
      "Epoch 15/50\n",
      "188/188 [==============================] - 56s 297ms/step - loss: 1.3433 - accuracy: 0.5785 - val_loss: 1.6771 - val_accuracy: 0.5033\n",
      "Epoch 16/50\n",
      "188/188 [==============================] - 52s 273ms/step - loss: 1.2889 - accuracy: 0.5930 - val_loss: 1.5622 - val_accuracy: 0.5233\n",
      "Epoch 17/50\n",
      "188/188 [==============================] - 51s 271ms/step - loss: 1.2726 - accuracy: 0.6034 - val_loss: 1.8646 - val_accuracy: 0.4754\n",
      "Epoch 18/50\n",
      "188/188 [==============================] - 50s 266ms/step - loss: 1.2709 - accuracy: 0.6033 - val_loss: 1.5109 - val_accuracy: 0.5502\n",
      "Epoch 19/50\n",
      "188/188 [==============================] - 50s 267ms/step - loss: 1.2901 - accuracy: 0.6001 - val_loss: 1.6259 - val_accuracy: 0.5273\n",
      "Epoch 20/50\n",
      "188/188 [==============================] - 54s 286ms/step - loss: 1.1650 - accuracy: 0.6368 - val_loss: 1.6919 - val_accuracy: 0.5219\n",
      "Epoch 21/50\n",
      "188/188 [==============================] - 54s 283ms/step - loss: 1.1209 - accuracy: 0.6415 - val_loss: 1.6363 - val_accuracy: 0.5279\n",
      "Epoch 22/50\n",
      "188/188 [==============================] - 60s 319ms/step - loss: 1.0367 - accuracy: 0.6692 - val_loss: 1.5869 - val_accuracy: 0.5542\n",
      "Epoch 23/50\n",
      "188/188 [==============================] - 60s 320ms/step - loss: 0.9886 - accuracy: 0.6857 - val_loss: 1.3641 - val_accuracy: 0.5977\n",
      "Epoch 24/50\n",
      "188/188 [==============================] - 59s 314ms/step - loss: 1.0632 - accuracy: 0.6619 - val_loss: 1.5895 - val_accuracy: 0.5422\n",
      "Epoch 25/50\n",
      "188/188 [==============================] - 58s 306ms/step - loss: 0.9720 - accuracy: 0.6955 - val_loss: 1.1721 - val_accuracy: 0.6672\n",
      "Epoch 26/50\n",
      "188/188 [==============================] - 102s 541ms/step - loss: 0.9877 - accuracy: 0.6890 - val_loss: 1.4290 - val_accuracy: 0.5981\n",
      "Epoch 27/50\n",
      "188/188 [==============================] - 62s 318ms/step - loss: 0.8071 - accuracy: 0.7416 - val_loss: 1.3110 - val_accuracy: 0.6253\n",
      "Epoch 28/50\n",
      "188/188 [==============================] - 775s 4s/step - loss: 0.8515 - accuracy: 0.7206 - val_loss: 1.5244 - val_accuracy: 0.5818\n",
      "Epoch 29/50\n",
      "188/188 [==============================] - 69s 367ms/step - loss: 0.8071 - accuracy: 0.7393 - val_loss: 1.0563 - val_accuracy: 0.6938\n",
      "Epoch 30/50\n",
      "188/188 [==============================] - 67s 357ms/step - loss: 0.8589 - accuracy: 0.7286 - val_loss: 1.1339 - val_accuracy: 0.6739\n",
      "Epoch 31/50\n",
      "188/188 [==============================] - 68s 359ms/step - loss: 0.6523 - accuracy: 0.7900 - val_loss: 1.1621 - val_accuracy: 0.6642\n",
      "Epoch 32/50\n",
      "188/188 [==============================] - 73s 389ms/step - loss: 0.7667 - accuracy: 0.7552 - val_loss: 1.0113 - val_accuracy: 0.7148\n",
      "Epoch 33/50\n",
      "188/188 [==============================] - 74s 391ms/step - loss: 0.7197 - accuracy: 0.7739 - val_loss: 1.4833 - val_accuracy: 0.6031\n",
      "Epoch 34/50\n",
      "188/188 [==============================] - 69s 368ms/step - loss: 0.7370 - accuracy: 0.7634 - val_loss: 0.9788 - val_accuracy: 0.7241\n",
      "Epoch 35/50\n",
      "188/188 [==============================] - 70s 371ms/step - loss: 0.6921 - accuracy: 0.7800 - val_loss: 0.8974 - val_accuracy: 0.7630\n",
      "Epoch 36/50\n",
      "188/188 [==============================] - 69s 364ms/step - loss: 0.4891 - accuracy: 0.8461 - val_loss: 0.9561 - val_accuracy: 0.7384\n",
      "Epoch 37/50\n",
      "188/188 [==============================] - 69s 363ms/step - loss: 0.6790 - accuracy: 0.7839 - val_loss: 0.9359 - val_accuracy: 0.7347\n",
      "Epoch 38/50\n",
      "188/188 [==============================] - 64s 340ms/step - loss: 0.6391 - accuracy: 0.8004 - val_loss: 1.0492 - val_accuracy: 0.7098\n",
      "Epoch 39/50\n",
      "188/188 [==============================] - 69s 363ms/step - loss: 0.5910 - accuracy: 0.8130 - val_loss: 1.0333 - val_accuracy: 0.7334\n",
      "Epoch 40/50\n",
      "188/188 [==============================] - 69s 368ms/step - loss: 0.6143 - accuracy: 0.8079 - val_loss: 1.7283 - val_accuracy: 0.6084\n",
      "Epoch 41/50\n",
      "188/188 [==============================] - 67s 354ms/step - loss: 0.5983 - accuracy: 0.8146 - val_loss: 0.8999 - val_accuracy: 0.7606\n",
      "Epoch 42/50\n",
      "188/188 [==============================] - 67s 357ms/step - loss: 0.5418 - accuracy: 0.8263 - val_loss: 1.0448 - val_accuracy: 0.7493\n",
      "Epoch 43/50\n",
      "188/188 [==============================] - 67s 357ms/step - loss: 0.5415 - accuracy: 0.8308 - val_loss: 0.8040 - val_accuracy: 0.7849\n",
      "Epoch 44/50\n",
      "188/188 [==============================] - 67s 353ms/step - loss: 0.6163 - accuracy: 0.8114 - val_loss: 0.7651 - val_accuracy: 0.8105\n",
      "Epoch 45/50\n",
      "188/188 [==============================] - 69s 365ms/step - loss: 0.4414 - accuracy: 0.8609 - val_loss: 1.1176 - val_accuracy: 0.7281\n",
      "Epoch 46/50\n",
      "188/188 [==============================] - 67s 354ms/step - loss: 0.5187 - accuracy: 0.8389 - val_loss: 0.8330 - val_accuracy: 0.7889\n",
      "Epoch 47/50\n",
      "188/188 [==============================] - 67s 356ms/step - loss: 0.4753 - accuracy: 0.8545 - val_loss: 0.8727 - val_accuracy: 0.7806\n",
      "Epoch 48/50\n",
      "188/188 [==============================] - 67s 355ms/step - loss: 0.6442 - accuracy: 0.8082 - val_loss: 0.7563 - val_accuracy: 0.8009\n",
      "Epoch 49/50\n",
      "188/188 [==============================] - 67s 356ms/step - loss: 0.5651 - accuracy: 0.8280 - val_loss: 1.2552 - val_accuracy: 0.6951\n",
      "Epoch 50/50\n",
      "188/188 [==============================] - 68s 361ms/step - loss: 0.4651 - accuracy: 0.8574 - val_loss: 0.8444 - val_accuracy: 0.7922\n"
     ]
    }
   ],
   "source": [
    "# Saving the model according to the conditions\n",
    "# checkpoint = ModelCheckpoint(\"Resnet_100.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "# Early Stopping is a regularization method (To minimize overfitting)\n",
    "early = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "history = resnet_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochsVal,\n",
    "    callbacks=[ early]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['training','validation'])\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['training','validation'])\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.show()\n",
    "score = resnet_model.evaluate(val_ds,verbose=0)\n",
    "print(\"Test score = \",score[0])\n",
    "print(\"Test Accuracy = \",score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
